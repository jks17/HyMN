{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d334def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from scipy import io as sio\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import (get_laplacian, to_scipy_sparse_matrix,\n",
    "                                   to_undirected, to_dense_adj)\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.special import comb\n",
    "\n",
    "from torch_geometric.nn.models import GIN\n",
    "from torch_geometric.utils.convert import to_networkx, from_networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca947cf6-1bde-4c03-aafd-38c53d3ee137",
   "metadata": {},
   "source": [
    "## Correlation with marking and Substructure Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be81d8c7-bb06-4062-8f46-5412a373c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GIN(2, 32, 3, 32)\n",
    "loss = torch.nn.L1Loss()\n",
    "\n",
    "graphs = []\n",
    "for _ in range(100):\n",
    "    graphs.append(nx.erdos_renyi_graph(20, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "833f52e4-cc67-44b0-a761-c8ba7ef16375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_paths(adjacency_matrix, cutoff):\n",
    "\n",
    "    # 0. instantiate Counter\n",
    "    counter = Counter({i: 0 for i in range(1,cutoff+1)})\n",
    "\n",
    "    # 0. get nx graph\n",
    "    G = nx.Graph(adjacency_matrix)\n",
    "\n",
    "    # 1. iterate over each node and count paths towards all other nodes with larger index\n",
    "    n = G.number_of_nodes()\n",
    "    for source in range(n):\n",
    "        targets = list(range(source+1, n))\n",
    "        paths = nx.all_simple_paths(G, source=source, target=targets, cutoff=cutoff)\n",
    "        path_lengths = list(map(lambda x: len(x)-1, paths))\n",
    "        counter.update(path_lengths)\n",
    "\n",
    "    # 2. arrange into list and return\n",
    "    return [counter[l] for l in sorted(counter)][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b88949c-ed3a-4833-9360-d27103afb512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/x5gc1ns91bqgspfwlbtsjq0c0000gp/T/ipykernel_79394/202353619.py:10: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  a = np.array(nx.adjacency_matrix(graph).todense())\n"
     ]
    }
   ],
   "source": [
    "num_triangles = []\n",
    "num_tailed = []\n",
    "num_cyc4 = []\n",
    "num_star = []\n",
    "num_2paths = []\n",
    "num_3paths = []\n",
    "num_4paths = []\n",
    "\n",
    "for graph in graphs:\n",
    "    a = np.array(nx.adjacency_matrix(graph).todense())\n",
    "    A2 = a.dot(a)\n",
    "    A3 = A2.dot(a)\n",
    "    tri = np.trace(A3) / 6\n",
    "    num_triangles.append(tri)\n",
    "    tailed = ((np.diag(A3) / 2) * (a.sum(0) - 2)).sum()\n",
    "    num_tailed.append(tailed)\n",
    "    cyc4 = 1 / 8 * (np.trace(A3.dot(a)) + np.trace(A2) - 2 * A2.sum())\n",
    "    num_cyc4.append(cyc4)\n",
    "    \n",
    "    deg = a.sum(0)\n",
    "    star = 0\n",
    "    for j in range(a.shape[0]):\n",
    "        star += comb(int(deg[j]), 3)\n",
    "    num_star.append(star)\n",
    "    \n",
    "    paths = count_paths(a, 4)\n",
    "    num_2paths.append(paths[0])\n",
    "    num_3paths.append(paths[1])\n",
    "    num_4paths.append(paths[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30548cc5-e311-4e52-90ed-e274bd207666",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.L1Loss()\n",
    "\n",
    "distances_random = []\n",
    "distances_max = []\n",
    "distances_min = []\n",
    "cent = []\n",
    "\n",
    "for graph in graphs:\n",
    "    data = from_networkx(graph)\n",
    "    data.x = torch.ones(len(G)).unsqueeze(dim=1)\n",
    "    v = torch.zeros(data.x.shape[0])\n",
    "    x = torch.cat((data.x, v.unsqueeze(dim=1)), dim=1)\n",
    "    \n",
    "    v = torch.zeros(data.x.shape[0])\n",
    "    G = to_networkx(data).to_undirected()\n",
    "    centralities = torch.tensor(list(nx.subgraph_centrality(G).values()))\n",
    "    rn = torch.argmax(centralities).item() \n",
    "    v[rn] = 1\n",
    "    x_ce = torch.cat((data.x, v.unsqueeze(dim=1)), dim=1)\n",
    "    \n",
    "    v = torch.zeros(data.x.shape[0])\n",
    "    rn = torch.argmin(centralities).item() \n",
    "    v[rn] = 1\n",
    "    x_ce_min = torch.cat((data.x, v.unsqueeze(dim=1)), dim=1)\n",
    "    \n",
    "    v = torch.zeros(data.x.shape[0])\n",
    "    rn = np.random.randint(len(v))\n",
    "    v[rn] = 1\n",
    "    x_random = torch.cat((data.x, v.unsqueeze(dim=1)), dim=1)\n",
    "\n",
    "    op = model(x, data.edge_index, data.batch)\n",
    "    p = op.clone()\n",
    "    op_ce = model(x_ce, data.edge_index, data.batch) \n",
    "    op_ce_min = model(x_ce_min, data.edge_index, data.batch) \n",
    "    op_random = model(x_random, data.edge_index, data.batch)\n",
    "    \n",
    "    distances_max.append((op.sum(dim=0) - op_ce.sum(dim=0)).abs().mean().item())\n",
    "    distances_min.append((op.sum(dim=0) - op_ce_min.sum(dim=0)).abs().mean().item())\n",
    "    distances_random.append((op.sum(dim=0) - op_random.sum(dim=0)).abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65efe870-d89d-42cc-b824-ead0f42af33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num triangles\n",
      "0.0051474159788623824\n",
      "-0.09413241218598847\n",
      "-0.17712497003936917\n",
      "num tailed\n",
      "-0.017225495286448796\n",
      "-0.09616413227253401\n",
      "-0.18094475420579376\n",
      "num 4 cycles\n",
      "-0.009213250542309084\n",
      "-0.03951341179394169\n",
      "-0.14532803797459062\n",
      "num star\n",
      "0.02544785729625549\n",
      "-0.03251029589563608\n",
      "-0.1419209343301108\n"
     ]
    }
   ],
   "source": [
    "substructures = [num_triangles, num_tailed, num_cyc4, num_star]\n",
    "name = ['num triangles', 'num tailed', 'num 4 cycles', 'num star']\n",
    "\n",
    "for idx, substructure in enumerate(substructures):\n",
    "    print(name[idx])\n",
    "    print(np.corrcoef(distances_max, substructure)[0][1])\n",
    "    print(np.corrcoef(distances_min, substructure)[0][1])\n",
    "    print(np.corrcoef(distances_random, substructure)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8d694-da5b-4fed-b970-c06c0f372642",
   "metadata": {},
   "source": [
    "## Correlation between different centralities and Subgraph Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fb782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBGRAPH_NUM_GRAPHS = 100\n",
    "parameters = [[60, 5], [60, 5], [60, 5], [60, 5]]\n",
    "largest_component = True\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "random_regular_graphs = []\n",
    "for i in range(SUBGRAPH_NUM_GRAPHS):\n",
    "    m, d = parameters[random_state.choice(len(parameters))]  # choose parameters uniformly to create regular graph\n",
    "    G = nx.random_regular_graph(d, m, seed=random_state)\n",
    "\n",
    "    edges = list(G.edges())\n",
    "    edges_to_remove = [edges[i] for i in random_state.choice(len(edges),m,replace=False)]  # remove m edges\n",
    "    for edge in edges_to_remove:\n",
    "        G.remove_edge(*edge)\n",
    "\n",
    "    if largest_component:\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        G_lcc = nx.convert_node_labels_to_integers(G.subgraph(largest_cc).copy())\n",
    "        random_regular_graphs.append(G_lcc)\n",
    "    else:\n",
    "        random_regular_graphs.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feac6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_pagerank = []\n",
    "correlation_betweeness = []\n",
    "correlation_degree = []\n",
    "correlation_closeness = []\n",
    "\n",
    "for G in random_regular_graphs:\n",
    "    sc = list(nx.subgraph_centrality(G).values())\n",
    "    pr = list(nx.pagerank(G).values())\n",
    "    dc = list(nx.degree_centrality(G).values())\n",
    "    cc = list(nx.closeness_centrality(G).values())\n",
    "    bc = list(nx.betweenness_centrality(G).values())\n",
    "    \n",
    "    correlation_pagerank.append(np.corrcoef(pr, sc)[0][1])\n",
    "    correlation_betweeness.append(np.corrcoef(bc, sc)[0][1])\n",
    "    correlation_degree.append(np.corrcoef(dc, sc)[0][1])\n",
    "    correlation_closeness.append(np.corrcoef(cc, sc)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0daec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9233506618482834\n",
      "0.02499154573303128\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(correlation_pagerank))\n",
    "print(np.std(correlation_pagerank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fcf231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8005609463351633\n",
      "0.07468393969910885\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(correlation_betweeness))\n",
    "print(np.std(correlation_betweeness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e88497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9704933467786431\n",
      "0.012401940735072663\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(correlation_degree))\n",
    "print(np.std(correlation_degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4c3529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7859225270314133\n",
      "0.06719529739829969\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(correlation_closeness))\n",
    "print(np.std(correlation_closeness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844df66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genesis",
   "language": "python",
   "name": "genesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
